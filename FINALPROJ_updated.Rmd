---
title: "FinalProj"
output: html_document
date: "2023-03-29"
---
Libraries
```{r}
library(tidyverse)
library(tidytext)
library(wordcloud)
library(topicmodels)
library(gridExtra)
library(shiny)
library(ggplot2)
library(ggmap)
library(tidyr)
library(stringr)
```

MAP DATA -  
```{r, Data Cleaning}
# Load in data
tweets <-  read_csv("Tweets-1.csv")

# Adding coordinates to the map
# Los Angeles
tweets$tweet_coord[tweets$tweet_location == "Los Angeles"] <- "[34.052235, -118.243683]"
tweets$tweet_coord[tweets$tweet_location == "Los Angeles, CA"] <- "[34.052235, -118.243683]"
tweets$tweet_coord[tweets$tweet_location == "Los Angeles, California"] <- "[34.052235, -118.243683]"
tweets$tweet_coord[tweets$tweet_location == "Los Angeles, Calif."] <- "[34.052235, -118.243683]"
tweets$tweet_coord[tweets$tweet_location == "Los Angeles, Ca"] <- "[34.052235, -118.243683]"
tweets$tweet_coord[tweets$tweet_location == "los angeles ca"] <- "[34.052235, -118.243683]"
tweets$tweet_coord[tweets$tweet_location == "los angeles, ca"] <- "[34.052235, -118.243683]"

# San Francisco
tweets$tweet_coord[tweets$tweet_location == "San Francisco CA"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "San Francisco, CA"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "San Francisco"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "san francisco"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "California, San Francisco"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "San Francisco Bay Area"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "san francisco califorania"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "San Francisco, California"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "San Francisco, Ca"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "san francisco, ca"] <- "[37.773972, -122.431297]"

# San Diego
tweets$tweet_coord[tweets$tweet_location == "San Diego"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "san diego"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "San Diego, CA"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "San Diego CA"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "San Diego, CA USA"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "San Diego, Ca"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "san diego, CA"] <- "[32.715736, -117.161087]"
tweets$tweet_coord[tweets$tweet_location == "San Diego, CA (760)"] <- "[32.715736, -117.161087]"

# New York City and New York
tweets$tweet_coord[tweets$tweet_location == "NYC"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "nyc"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "New York City"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "New York City, NY"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "new york city"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "New York, NY"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "new york, new york"] <- "[40.730610, -73.935242]"
tweets$tweet_coord[tweets$tweet_location == "New York, New York"] <- "[40.730610, -73.935242]"

# Brooklyn 
tweets$tweet_coord[tweets$tweet_location == "Brooklyn"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "Brooklyn, NY"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "brooklyn, Ny"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "Brooklyn, New York"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "Brooklyn NY"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "brooklyn, ny, us"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "Brooklyn NY."] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "brooklyn"] <- "[40.650002, -73.949997]"
tweets$tweet_coord[tweets$tweet_location == "Brooklyn!"] <- "[40.650002, -73.949997]"

# Boston 
tweets$tweet_coord[tweets$tweet_location == "Boston, MA"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "Boston"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "Boston, Massachusetts"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "boston, ma"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "boston ma"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "Boston, MA."] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "Boston, USA"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "BOSTON"] <- "[42.361145, -71.057083]"
tweets$tweet_coord[tweets$tweet_location == "Boston, MA"] <- "[42.361145, -71.057083]"

# Rest of California
tweets$tweet_coord[tweets$tweet_location == "Stockton, CA"] <- "[37.961632, -121.275604]"
tweets$tweet_coord[tweets$tweet_location == "palo alto, ca"] <- "[37.468319, -122.143936]"
tweets$tweet_coord[tweets$tweet_location == "Manhattan Beach, CA."] <- "[33.881248, -118.407211]"
tweets$tweet_coord[tweets$tweet_location == "Oakland, California"] <- "[37.804363, -122.271111]"
tweets$tweet_coord[tweets$tweet_location == "Silicon Valley, California"] <- "[37.387474, -122.057543]"
tweets$tweet_coord[tweets$tweet_location == "Anaheim, CA"] <- "[33.835293, -117.914505]"
tweets$tweet_coord[tweets$tweet_location == "Malibu, CA"] <- "[34.031246, -118.788193]"
tweets$tweet_coord[tweets$tweet_location == "Long Beach, CA"] <- "[33.770050, -118.193741]"
tweets$tweet_coord[tweets$tweet_location == "San Marcos, CA"] <- "[33.143372, -117.166145]"
tweets$tweet_coord[tweets$tweet_location == "San Jose, California"] <- "[37.335480, -121.893028]"
tweets$tweet_coord[tweets$tweet_location == "Oakland, CA"] <- "[37.804363, -122.271111]"
tweets$tweet_coord[tweets$tweet_location == "Oakland, Ca"] <- "[37.804363, -122.271111]"
tweets$tweet_coord[tweets$tweet_location == "Rocklin, CA"] <- "[38.800011, -121.246731]"
tweets$tweet_coord[tweets$tweet_location == "Santa Rosa, CA"] <- "[38.444660, -122.720306]"
tweets$tweet_coord[tweets$tweet_location == "SF, CA"] <- "[37.773972, -122.431297]"
tweets$tweet_coord[tweets$tweet_location == "Calabasas, CA"] <- "[34.138332, -118.660835]"
tweets$tweet_coord[tweets$tweet_location == "Sacramento, CA"] <- "[38.575764, -121.478851]"
tweets$tweet_coord[tweets$tweet_location == "Hermosa Beach, CA"] <- "[33.862141, -118.400009]"
tweets$tweet_coord[tweets$tweet_location == "Mountain View, CA"] <- "[37.386051, -122.083855]"
tweets$tweet_coord[tweets$tweet_location == "Venice, CA"] <- "[33.988270, -118.472023]"
tweets$tweet_coord[tweets$tweet_location == "Foster City, CA"] <- "[37.55855, -122.27108]"
tweets$tweet_coord[tweets$tweet_location == "Fresno, CA"] <- "[36.746841, -119.772591]"
tweets$tweet_coord[tweets$tweet_location == "Aliso Viejo, CA"] <- "[33.567684, -117.725609]"
tweets$tweet_coord[tweets$tweet_location == "Culver City, CA"] <- "[34.010124, -118.415016]"
tweets$tweet_coord[tweets$tweet_location == "San Jose, CA"] <- "[37.335480, -121.893028]"
tweets$tweet_coord[tweets$tweet_location == "santa barbara, CA"] <- "[34.420830, -119.698189]"
tweets$tweet_coord[tweets$tweet_location == "Irvine, CA"] <- "[33.669445, -117.823059]"
tweets$tweet_coord[tweets$tweet_location == "Costa Mesa, CA"] <- "[33.641132, -117.918671]"
tweets$tweet_coord[tweets$tweet_location == "Mill Valley, CA"] <- "[37.905182, -122.544006]"
tweets$tweet_coord[tweets$tweet_location == "Fremont, California"] <- "[37.548271, -121.988571]"
tweets$tweet_coord[tweets$tweet_location == "Brentwood, CA"] <- "[37.931868, -121.695786]"
tweets$tweet_coord[tweets$tweet_location == "Berkeley, CA"] <- "[37.871666, -122.272781]"
tweets$tweet_coord[tweets$tweet_location == "San Bruno, CA"] <- "[37.625288, -122.425266]"
tweets$tweet_coord[tweets$tweet_location == "Ventura, California"] <- "[34.274647, -119.229034]"
tweets$tweet_coord[tweets$tweet_location == "Fullerton, CA"] <- "[33.876118, -117.921410]"
tweets$tweet_coord[tweets$tweet_location == "Redlands, CA"] <- "[34.055569, -117.182541]"
tweets$tweet_coord[tweets$tweet_location == "Hollywood, CA"] <- "[34.098907, -118.327759]"
tweets$tweet_coord[tweets$tweet_location == "Roseville, CA"] <- "[38.752125, -121.288010]"
tweets$tweet_coord[tweets$tweet_location == "Campbell, CA"] <- "[37.287167, -121.949959]"
tweets$tweet_coord[tweets$tweet_location == "Santa Ana, California"] <- "[33.745571, -117.867836]"
tweets$tweet_coord[tweets$tweet_location == "Union City, CA"] <- "[37.593392, -122.04383]"
tweets$tweet_coord[tweets$tweet_location == "Beverly Hills, CA"] <- "[34.073620, -118.400352]"
tweets$tweet_coord[tweets$tweet_location == "Pescadero, CA"] <- "[37.255164, -122.383015]"
tweets$tweet_coord[tweets$tweet_location == "Sherman Oaks, CA"] <- "[34.149956, -118.448891]"
tweets$tweet_coord[tweets$tweet_location == "Atascadero, CA"] <- "[35.489418, -120.670723]"
tweets$tweet_coord[tweets$tweet_location == "Rialto, CA"] <- "[34.106117, -117.372093]"
tweets$tweet_coord[tweets$tweet_location == "Palm Springs, CA"] <- "[33.830517, -116.545601]"
tweets$tweet_coord[tweets$tweet_location == "Pasadena, CA"] <- "[34.156113, -118.131943]"
tweets$tweet_coord[tweets$tweet_location == "Newport Beach, CA"] <- "[33.628342, -117.927933]"
tweets$tweet_coord[tweets$tweet_location == "Napa, CA"] <- "[38.297539, -122.286865]"
tweets$tweet_coord[tweets$tweet_location == "Redwood City, CA"] <- "[37.487846, -122.236115]"
tweets$tweet_coord[tweets$tweet_location == "West Covina, CA"] <- "[34.068623, -117.938950]"

# Further cleaning of data for map analysis
tweets <- tweets %>% drop_na(tweet_coord)

tweets$tweet_coord <- gsub("\\[|\\]", "", tweets$tweet_coord)

tweets <- tweets %>% separate_wider_delim(tweet_coord, ",",names = c("Lat", "Lon")) 

tweets$Lat <- as.numeric(tweets$Lat)
tweets$Lon <- as.numeric(tweets$Lon)
tweets$airline <- as.factor(tweets$airline)
tweets$airline_sentiment <- as.factor(tweets$airline_sentiment)
```

```{r, Create Maps}
# World, US, California, and New York Maps
world.map <- get_map(location = "United States", zoom = 1)
worldmap <- ggmap(world.map)
worldmap

us.map <- get_map(location = "United States", zoom = 4)
USmap <- ggmap(us.map)
USmap

california.map <- get_map(location = "California", zoom = 6)
californiamap <- ggmap(california.map)
californiamap

newyork.map <- get_map(location = "New York", zoom = 7)
newyorkmap <- ggmap(newyork.map)
newyorkmap
```

```{r, World Maps}
worldtweetmap <- worldmap + geom_point(data = tweets, aes(x = Lon, y = Lat))
worldtweetmap

worldairlinemap <- worldmap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline), alpha = 0.6)
worldairlinemap

worldsentimentmap <- worldmap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline_sentiment), alpha = 0.6)
worldsentimentmap
```

```{r, US Maps}
ustweetmap <- USmap + geom_point(data = tweets, aes(x = Lon, y = Lat))
ustweetmap

usairlinemap <- USmap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline))
usairlinemap

ussentimentmap <- USmap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline_sentiment))
ussentimentmap
```

```{r, California Maps}
calitweetmap <- californiamap + geom_point(data = tweets, aes(x = Lon, y = Lat))
calitweetmap

calisentimentmap <- californiamap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline_sentiment), alpha = 0.5, size = 4) 
calisentimentmap

caliairlinemap <- californiamap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline), size = 4, alpha = 0.7)
caliairlinemap
```

```{r, New York Maps}
newyorktweetmap <- newyorkmap + geom_point(data = tweets, aes(x = Lon, y = Lat), size = 2)
newyorktweetmap

nyairlinemap <- newyorkmap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline), alpha = 0.8, size = 4)
nyairlinemap

nysentimentmap <- newyorkmap + geom_point(data = tweets, aes(x = Lon, y = Lat, color = airline_sentiment), alpha = 0.5, size = 4)
nysentimentmap
```

SENTIMENT ANALYSIS-

```{r, Data Prep}
# Load Data
tweets = read_csv("Tweets-1.csv")

#Tokenize and Remove Stop Words

custom_stop_words <- tribble(
  # Column names should match stop_words
  ~word, ~lexicon,
  # Add http, win, and t.co as custom stop words
  "http", "CUSTOM",
  "win", "CUSTOM",
  "t.co", "CUSTOM",
  "virginamerica", "CUSTOM",
  "sfo", "CUSTOM",
  "united", "CUSTOM",
  "southwestair", "CUSTOM",
  "americanair", "CUSTOM",
  "usairways","CUSTOM",
  "jetblue","CUSTOM",
  "flight", "CUSTOM",
  "flights","CUSTOM",
  "airline","CUSTOM",
  "fly","CUSTOM",
  "guys","CUSTOM"
)
# Bind the custom stop words to stop_words
stop_words2 <- stop_words %>% 
   bind_rows(custom_stop_words)
#
tidy_twitter <- tweets %>% 
  
  select_if(~!any(is.na(.)))  %>% 
  # Tokenize the twitter data
  unnest_tokens(word, text) %>% 
  # Remove stop words
  anti_join(stop_words2) %>%
  # Remove incoherent words and numbers using grepl
  filter(!grepl("\\b\\d+\\b|[^[:alpha:]]", word)) %>%
 
  filter(nchar(word) > 2)


```

```{r, Visualisation without a Sentiment Dictionary}

#General sentiment count
tidy_twitter %>% count(airline_sentiment)

#Most frequent words in neutral
word_counts <- tidy_twitter %>% 
  filter(airline_sentiment == "neutral") %>% 
  count(word) %>%  
  filter(n > 50) %>% 
  arrange(desc(n))  

ggplot(word_counts, aes(reorder(word, n), n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Count")+ggtitle("neutral")

#Most frequent words in Positive
word_counts <- tidy_twitter %>% 
  filter(airline_sentiment == "positive") %>% 
  count(word) %>%  
  filter(n > 50) %>% 
  arrange(desc(n))  

ggplot(word_counts, aes(reorder(word, n), n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Count")+ggtitle("positive")

#Most frequent words in negative
word_counts <- tidy_twitter %>% 
  filter(airline_sentiment == "negative") %>% 
  count(word) %>%  
  filter(n > 400) %>% 
  arrange(desc(n))  

ggplot(word_counts, aes(reorder(word, n), n)) +
  geom_col() +
  coord_flip() +
  labs(x = "Word", y = "Count")+ggtitle("Negative")

# Top 20 Sentiments compared
word_counts <- tidy_twitter %>%
  # Count words by whether or not its a complaint
  count(word, airline_sentiment) %>%
  # Group by whether or not its a complaint
  group_by(airline_sentiment) %>%
  # Keep the top 20 words
  slice_max(n, n = 20) %>%
  # Ungroup before reordering word as a factor by the count
  ungroup() %>%
  mutate(word2 = fct_reorder(word, n))

ggplot(word_counts, aes(x = word2, y = n, fill  = airline_sentiment)) +
  # Don't include the lengend for the column plot
  geom_col(show.legend = FALSE) +
  # Facet by whether or not its a complaint and make the y-axis free
  facet_wrap(~airline_sentiment, scales = "free_y") +
  # Flip the coordinates and add a title: "Twitter Word Counts"
   coord_flip() +
   ggtitle("Sentiment Comparison") 
#Frequency od tweets for each airline 
tidy_twitter %>% count(airline)
#Sentiment Count by Each Airline
aa_count <- tidy_twitter %>%
  group_by(airline, airline_sentiment) %>%
  summarise(count = n()) %>%
  ungroup()

ggplot(aa_count, aes(x = airline, y = count, fill = airline_sentiment)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Airline", y = "Sentiment Count", title = "Sentiment Count by Each Airline") +
  theme_minimal()
```

```{r, Word Cloud}
#Word Cloud
word_counts <- tidy_twitter %>% 
  count(word)

# Define the color palette with more interpolated colors
color_palette <- colorRampPalette(brewer.pal(8, "Dark2"))(100)

wordcloud(
  words = word_counts$word,
  freq = word_counts$n,
  max.words = 100,
  colors = color_palette,
  random.order = FALSE,
  rot.per = 0.35,
  scale = c(3, 0.5)
)

```

```{r, Sentiment Dictionary vs Airline_sentiments}
#Sorting the original data using the dictionary

sentiment_twitter <- tidy_twitter %>% 
  inner_join(get_sentiments("nrc"))
 

  # Count by complaint label and sentiment
 count1 = sentiment_twitter  %>% count(airline_sentiment, sentiment) %>% 
  # Spread the sentiment and count columns
  pivot_wider(names_from = sentiment,values_from = n)

#The words in the dataframe are grouped by the sentiments Potitive, negative and neutral. The graph shows how many of the NRC sentiment words are in the original sentiments (Load sentiment_melted dataframe for more context)
sentiment_melted <- count1 %>%
  tidyr::pivot_longer(cols = -airline_sentiment, names_to = "sentiment", values_to = "count")

# Create a bar plot
ggplot(sentiment_melted, aes(x = airline_sentiment, y = count, fill = sentiment)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Airline Sentiment Type(provided by the original Data)", y = "Airline Sentiment Frequency ", title = "NRC sentiments vs Airline Sentiment", fill = "NRC Dictionary Sentiments") +
  theme_minimal()

#Since there are more categories in NRC sentiment Dictionary, the distribution of positive and negative words are different in sentiment_twitter df as compared to the original data frame, tidy_twitter. According to the original sentiments provided by the airline_sentiment column (in tidy_twitter df), there are more negative words than positive words. However according to the NRC dictionary (sentiment_twitter df), positive words have the most frequency. This is just an illusion because in both these data frames, sentiments are categorized differently. NRC dictionary has a lot more categories, thus splitting the original negative words list into more categories. Another reason this is happening is because the sentiment_twitter df is half the size of the original dataframe.There's a graph below highlighting the discrepancy 

####

```

```{r, Data Discrepancy(read above chunk)}

word_count <- tidy_twitter %>% 
  count(airline_sentiment, sort = TRUE)

# Create a simple ggplot of word count
g1 = ggplot(word_count, aes(x = airline_sentiment, y = n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Words", y = "Count") +
  ggtitle("Without NRC dictionary")

###
word_count1 <- sentiment_twitter %>% 
  count(sentiment) %>%
  arrange(desc(n))

# Create a simple ggplot of word count in descending order
g2 = ggplot(word_count1, aes(x = reorder(sentiment, -n), y = n)) +
  geom_col() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(x = "Words", y = "Count") +
  ggtitle("With NRC dictionary")

grid.arrange(g1,g2,ncol = 2)

#NRC Sentiment count by Each airline

ab_count <- sentiment_twitter %>%
  group_by(airline, sentiment) %>%
  summarise(count = n()) %>%
  ungroup()

ggplot(ab_count, aes(x = airline, y = count, fill = sentiment)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(x = "Airline", y = "Sentiment Count", title = "Sentiment Count by Each Airline") +
  theme_minimal()

```

```{r, Topic Modeling and Pattern Analysis}

#Topic modeling is a probabilistic model used to extract hidden patterns in a large collection of text. (the specifics are in datacamp -> "Introduction to Text Analysis in R" course ->  Topic modeling (last section)-> "Interpreting Topics" video)

dtm_twitter <- tidy_twitter %>% 
  count(word, tweet_id) %>% 
  # Cast the word counts by tweet into a DTM
  cast_dtm(tweet_id, word, n)

# Coerce dtm_twitter into a matrix called matrix_twitter
matrix_twitter <- as.matrix(dtm_twitter)

##
lda_out2 <- LDA(
  dtm_twitter,  k = 3,  method = "Gibbs",
  control = list(seed = 42)
)

# Tidy the matrix of word probabilities
lda_topics2 <- lda_out2 %>%   tidy(matrix = "beta")



#
word_probs2 <- lda_topics2 %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 15) %>% 
  ungroup() %>%
  mutate(term2 = fct_reorder(term, beta))

# Plot word probs, color and facet based on topic
ggplot(
  word_probs2, 
  aes(term2, beta, fill = as.factor(topic))
) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

```

 SHINY -

```{r, UI}

ui <- fluidPage(
  titlePanel("Airline Sentiment Analysis"),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("x_axis", "Choose X-axis variable:",
                  choices = c("word", "airline_sentiment"),
                  selected = "word"),
      selectInput("y_axis", "Choose Y-axis variable:",
                  choices = c("n"),
                  selected = "n"),
      conditionalPanel(
        condition = "input.x_axis == 'word'",
        actionButton("word_cloud", "Show Word Cloud")
      )
    ),
    
    mainPanel(
      plotOutput("sentimentPlot"),
      plotOutput("wordCloudPlot", height = "400px")
    )
  )
)


```

```{r, Server}
server <- function(input, output) {
  
  output$sentimentPlot <- renderPlot({
    
    if (input$x_axis == "word") {
      word_counts <- tidy_twitter %>%
        count(word, airline_sentiment) %>%
        filter(n > 100) %>%
        arrange(desc(n)) %>%
        head(100)
      
      ggplot(word_counts, aes_string(x = input$x_axis, y = input$y_axis, fill = "airline_sentiment")) +
        geom_col() +
        coord_flip() +
        labs(x = input$x_axis, y = input$y_axis)
    } else {
      sentiment_counts <- tidy_twitter %>%
        count(airline_sentiment) %>%
        arrange(desc(n))
      
      ggplot(sentiment_counts, aes_string(x = input$x_axis, y = input$y_axis)) +
        geom_col() +
        labs(x = input$x_axis, y = input$y_axis)
    }
  })
  
  output$wordCloudPlot <- renderPlot({
    if (input$word_cloud > 0) {
      word_counts <- tidy_twitter %>%
        count(word) %>%
        head(100)
      
      color_palette <- colorRampPalette(brewer.pal(8, "Dark2"))(300)
      
      wordcloud(
        words = word_counts$word,
        freq = word_counts$n,
        max.words = 100,
        colors = color_palette,
        random.order = FALSE,
        rot.per = 0.35,
        scale = c(3, 0.5)
      )
    } else {
      return(NULL)
    }
  })
}



```

```{r, Run App (Without NRC)}
shinyApp(ui = ui, server = server)

```

```{r, UI2}
ui2 <- fluidPage(
  titlePanel("Sentiment Twitter Analysis"),
  
  sidebarLayout(
    sidebarPanel(
      h3("Sentiment Analysis based on NRC Dictionary"),
      selectInput("x_axis2", "Choose X-axis variable:",
                  choices = c("word", "sentiment"),
                  selected = "word"),
      selectInput("y_axis2", "Choose Y-axis variable:",
                  choices = c("sentiment", "word"),
                  selected = "sentiment")
    ),
    
    mainPanel(
      plotOutput("sentimentBarPlot2")
    )
  )
)



```

```{r, Server2}
server2 <- function(input, output) {
  
  output$sentimentBarPlot2 <- renderPlot({
    
    if (input$x_axis2 == "word") {
      top_words <- sentiment_twitter %>%
        count(word, sentiment) %>%
        group_by(sentiment) %>%
        top_n(20, n) %>%
        ungroup()

      ggplot(top_words, aes(x = reorder(word, n), y = n, fill = sentiment)) +
        geom_col() +
        coord_flip() +
        labs(x = "Word", y = "Count") +
        facet_wrap(~ sentiment, scales = "free_y")
    } else {
      sentiment_counts <- sentiment_twitter %>%
        count(sentiment) %>%
        arrange(desc(n))

      ggplot(sentiment_counts, aes(x = sentiment, y = n)) +
        geom_col() +
        labs(x = "Sentiment", y = "Count")
    }
  })
}



```

```{r, Run App (With NRC Dictionary)}
shinyApp(ui = ui2, server = server2)

```
